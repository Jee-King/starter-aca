<!DOCTYPE html>
<html>
<title>FE108: A large scale frame-event-based object tracking dataset</title>

<head>
  <meta charset="UTF-8" />
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css" />
  <link rel="stylesheet" href="../css/index.css" />
  <script src="https://unpkg.com/vue/dist/vue.js"></script>
  <script src="https://unpkg.com/element-ui/lib/index.js"></script>
  <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
</head>

<body>
  <div id="app">
    <el-page-header @back="goBack" title="Back"> </el-page-header>
    <div>
      <h1>Intelligent Bionic System</h1>
      <el-divider></el-divider>
      <p>
        The uncertain scene object refers to the object which brings uncertainty to the scene understanding process for
        intelligence systems. The presence of uncertain objects in the scene severely affects intelligent decisions in
        many applications such as robotic navigation and drone tracking. In this project, we are developing techniques
        for uncertain object detection/segmentation. Based on environmental data perception, we are committed to mining
        the features and patterns of scene data from different perspectives such as computational statistics, behavioral
        cognition, and semantics, combining multi-dimensional and multi-modal technical means, so as to realize
        effective analysis, cognition, and expression of the uncertain scene object. Specifically, the uncertain objects
        we focus on include special, camouflaged, and salient objects in the scene.
      </p>
      <p>
        Currently, we mainly build our Intelligent Bionic System from three aspects. Firstly, from the perceptual level, we study bio-inspired vision sensors – Event Cameras. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the output is composed of a sequence of asynchronous events rather than actual intensity images, traditional vision algorithms cannot be applied, so that new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required.
      </p>
      <p>
        Secondly, from the algorithms level, we explore the Spiking Neural Networks (SNNs) which is a bio-inspired model consisting of spiking neurons as the computational model. Compared with ANNs, SNNs has the advantages of fast response, low energy consumption and strong anti-noise ability. However, the back propagation mechanism and impulse plasticity of spiking neural networks are still unclear.
      </p>
      <p>
        Thirdly, from the application level, we explore the effective combination of bionic sensors, algorithms and chips to complete practical tasks with specific needs.
      </p>
      <el-divider></el-divider>
      <div v-for="project,index in projects">
        <p class="project_p">
          <span v-html="textBold(project.authors)"></span>
          <b class="project_title">{{project.title}}</b>
          <span v-for="tag in project.tags" :style="styles[tag[1]]">{{tag[0]}}</span>
          <span v-for="link in project.links">[<b><a class="project_links"
                :href="link.link">{{link.name}}</a></b><span>]&nbsp;</span></span>
        </p>
        <div v-for="img in project.imgs" class="figure_div">
          <img :src="'figures/'+img.src" :alt="img" class="figure_img" />
          <div v-if="img.label">
            <div class="figure_label">{{img.label}}</div>
            <br>
          </div>
        </div>
        <div v-if="project.io">
          <p><b>Input-Output:&nbsp;</b>{{project.io}}<span></span></p>
        </div>
        <div v-if="project.abstract">
          <p><b>Abstract.&nbsp;</b>{{project.abstract}}<span></span></p>
        </div>
        <el-divider content-position="right">{{index+1}}</el-divider>
      </div>
    </div>
  </div>
</body>
<script src="../js/index.js"></script>
<script>
  new Vue({
    el: '#app',
    data: function () {
      return {
        styles,
        projects: [
          {
            authors: 'Jiqing Zhang, Xin Yang, Yingkai Fu, Xiaopeng Wei, Baocai Yin, Bo Dong.',
            title: 'Object Tracking by Jointly Exploiting Frame and Event Domain.',
            tags: [
              ['ICCV 2021', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            abstract: 'Inspired by the complementarity between conventional frame-based and bio-inspired event-based cameras, we propose a multi-modal based approach to fuse visual cues from the frame- and event-domain to enhance the single object tracking performance, especially in degraded conditions (e.g., scenes with high dynamic range, low light, and fast motion objects). The proposed approach can effectively and adaptively combine meaningful information from both domains. Our approach’s effectiveness is enforced by a novel designed cross-domain attention schemes, which can effectively enhance features based on self- and cross-domain attention schemes; The adaptiveness is guarded by a specially designed weighting scheme, which can adaptively balance the contribution of the two domains. To exploit event-based visual cues in single-object tracking, we construct a largescale frame-event-based dataset, which we subsequently employ to train a novel frame-event fusion based model. Extensive experiments show that the proposed approach outperforms state-of-the-art frame-based tracking methods by at least 10.4% and 11.9% in terms of representative success rate and precision rate, respectively. Besides, the effectiveness of each key component of our approach is evidenced by our thorough ablation study.',
            imgs: [
              { src: 'Object-Tracking-1.png' }
            ],
          },
          {
            authors: 'Jiqing Zhang, Kai Zhao, Bo Dong, Yingkai Fu, Xinglin Piao, Xin Yang, Baocai Yin.',
            title: 'Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking.',
            tags: [
              ['The Visual Computer (Proc. CGI 2021)', 1],
              ['. ', 0]
            ],
            abstract: 'Jointly exploiting multiple different yet complementary domain information has been proven to be an effective way to perform robust object tracking. This paper focuses on effectively representing and utilizing complementary features from the frame domain and event domain for boosting object tracking performance in challenge scenarios. Specifically, we propose Common Features Extractor (CFE) to learn potential common representations from the RGB domain and event domain. For learning the unique features of the two domains, we utilize a Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract edge cues in the event domain which may be missed in RGB in some challenging conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional Neural Networks to extract texture and semantic information in RGB domain. Extensive experiments on standard RGB benchmark and real event tracking dataset demonstrate the effectiveness of the proposed approach. We show our approach outperforms all compared state-of-the-art tracking algorithms and verify event-based data is a powerful cue for tracking in challenging scenes.',
            imgs: [
              { src: 'Multi-domain-1.png' }
            ],
          },
        ]
      }
    },
    methods: {
      textBold(s) {
        return textBold(s)
      },
      goBack() {
        goBack()
      },
    },
  })
</script>
